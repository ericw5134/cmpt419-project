{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28db565-2ac1-4813-9804-7696e076e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "from utility.data_loader import EmotionDataset\n",
    "from utility.models import VideoCNN, AudioCNN\n",
    "from utility.fusion import LateFusion\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165d63c-22ee-4e5d-befd-85022bf5d7a2",
   "metadata": {},
   "source": [
    "# Training Model Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc59f74-0499-4a8e-b187-ad30f4d8bf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'data/train' \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((112, 112)), #resize should happen before ToTensor.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "audio_transform = MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=512, n_mels=64)\n",
    "\n",
    "dataset = EmotionDataset(root_dir, transform=transform, audio_transform=audio_transform)\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8168769f-a7f7-43d3-9526-bc41a698c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\ericw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Loss: 4.3384 | Acc: 39.44%\n",
      "Epoch 2/20 | Loss: 3.5869 | Acc: 54.93%\n",
      "Epoch 3/20 | Loss: 5.9886 | Acc: 52.82%\n",
      "Epoch 4/20 | Loss: 6.6508 | Acc: 54.93%\n",
      "Epoch 5/20 | Loss: 3.6231 | Acc: 52.82%\n",
      "Epoch 6/20 | Loss: 3.5894 | Acc: 60.56%\n",
      "Epoch 7/20 | Loss: 3.2959 | Acc: 61.27%\n",
      "Epoch 8/20 | Loss: 3.6204 | Acc: 58.45%\n",
      "Epoch 9/20 | Loss: 4.3582 | Acc: 57.04%\n",
      "Epoch 10/20 | Loss: 2.8948 | Acc: 69.01%\n",
      "Epoch 11/20 | Loss: 3.2248 | Acc: 64.79%\n",
      "Epoch 12/20 | Loss: 3.2135 | Acc: 61.97%\n",
      "Epoch 13/20 | Loss: 3.8234 | Acc: 62.68%\n",
      "Epoch 14/20 | Loss: 2.8097 | Acc: 66.90%\n",
      "Epoch 15/20 | Loss: 2.9723 | Acc: 64.79%\n",
      "Epoch 16/20 | Loss: 4.6735 | Acc: 61.27%\n",
      "Epoch 17/20 | Loss: 3.6207 | Acc: 69.72%\n",
      "Epoch 18/20 | Loss: 3.3436 | Acc: 68.31%\n",
      "Epoch 19/20 | Loss: 2.6212 | Acc: 71.13%\n",
      "Epoch 20/20 | Loss: 2.3408 | Acc: 74.65%\n"
     ]
    }
   ],
   "source": [
    "video_model = VideoCNN(num_classes=2).to(device)\n",
    "audio_model = AudioCNN(num_classes=2).to(device)\n",
    "fusion_model = LateFusion(num_classes=2, fusion_type='mlp').to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    list(video_model.parameters()) +\n",
    "    list(audio_model.parameters()) +\n",
    "    list(fusion_model.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# training\n",
    "num_epochs = 20\n",
    "# num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    video_model.train()\n",
    "    audio_model.train()\n",
    "    fusion_model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for spectrograms, frames, labels in dataloader:\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        frames = frames.to(device)\n",
    "        \n",
    "        # label values\n",
    "        # print(\"Batch label values:\", labels)\n",
    "        # print(\"Unique:\", labels.unique())\n",
    "\n",
    "        labels = labels.long().to(device)  # Ensure correct type\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        video_logits = video_model(frames)           \n",
    "        audio_logits = audio_model(spectrograms)     \n",
    "        fused_logits = fusion_model(video_logits, audio_logits)  \n",
    "\n",
    "        loss = criterion(fused_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(fused_logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | Acc: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0cc5dc-144c-4850-b7c9-7c30106aab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'video_model': video_model.state_dict(),\n",
    "    'audio_model': audio_model.state_dict(),\n",
    "    'fusion_model': fusion_model.state_dict()\n",
    "}, \"rage_detection_fusion_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0518d8d9-8e6c-4e58-a18a-da319e03158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(video_model, audio_model, fusion_model, dataloader, device):\n",
    "    video_model.eval()\n",
    "    audio_model.eval()\n",
    "    fusion_model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, frames, labels in dataloader:\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            video_logits = video_model(frames)\n",
    "            audio_logits = audio_model(spectrograms)\n",
    "            fused_logits = fusion_model(video_logits, audio_logits)\n",
    "\n",
    "            predictions = torch.argmax(fused_logits, dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n Evaluation Metrics:\")\n",
    "    print(f\"Accuracy     : {acc:.4f}\")\n",
    "    print(f\"Precision    : {prec:.4f}\")\n",
    "    print(f\"Recall       : {rec:.4f}\")\n",
    "    print(f\"F1-score     : {f1:.4f}\")\n",
    "\n",
    "    # plot the confusion matrix\n",
    "    class_names = [\"Rage\", \"Not Rage\"]\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Evaluation metrics with both classses:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c68ede1-6057-4c67-bff9-e6c17aad1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training data (not really necessary)\n",
    "# evaluate_model(video_model, audio_model, fusion_model, dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f51cc-30b0-4515-891f-2ceab4d385d1",
   "metadata": {},
   "source": [
    "# Testing Model Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ed364d-1a8f-4dd2-9849-c4113b4eab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'data/test' \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((112, 112)), #resize should happen before ToTensor.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "audio_transform = MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=512, n_mels=64)\n",
    "\n",
    "dataset = EmotionDataset(root_dir, transform=transform, audio_transform=audio_transform)\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a8edc0-7bdb-4493-8645-f2be98ecfb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data if necessary\n",
    "# Comment this block out if finished training and models are still in memory, otherwise you reset the weights or load from previous.\n",
    "video_model = VideoCNN(num_classes=2).to(device)\n",
    "audio_model = AudioCNN(num_classes=2).to(device)\n",
    "fusion_model = LateFusion(num_classes=2, fusion_type='mlp').to(device)\n",
    "\n",
    "saved = torch.load(\"rage_detection_fusion_train.pt\", weights_only=True)\n",
    "video_model.load_state_dict(saved[\"video_model\"])\n",
    "audio_model.load_state_dict(saved[\"audio_model\"])\n",
    "fusion_model.load_state_dict(saved[\"fusion_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14417f-7c19-4740-8308-fa0e804290f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(video_model, audio_model, fusion_model, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89b72a-7b20-4ae7-ace5-c2afe35608a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
