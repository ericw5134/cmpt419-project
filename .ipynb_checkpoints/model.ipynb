{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d23fb55-9e80-44ab-9872-5168e88c4e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T23:48:19.956954500Z",
     "start_time": "2025-03-31T23:48:15.230787600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio.transforms as T\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utility.data_loader import EmotionDataset\n",
    "from utility.fusion import LateFusion\n",
    "from utility.models import AudioCNN, VideoCNN\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ecce9a-a70c-4f75-8d23-b20f26871783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T23:48:20.014384400Z",
     "start_time": "2025-03-31T23:48:19.958954900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b78e8f-4da0-4af9-b004-2095fa92ef67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T23:48:20.014384400Z",
     "start_time": "2025-03-31T23:48:20.003959500Z"
    }
   },
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "audio_transform = transforms.Compose([\n",
    "    T.MelSpectrogram(\n",
    "        sample_rate=16000,\n",
    "        n_mels=64,\n",
    "        n_fft=1024,\n",
    "        hop_length=512\n",
    "    ),\n",
    "    transforms.Resize((64, 64)) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d34433a6d51d7f2b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "Absolute path: C:\\Users\\ahmad\\PycharmProjects\\cmpt419-project\\data\\train\\Audio\\eric_excitement1\\eric_excitement1.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the first audio file exists\n",
    "audio_path = os.path.normpath(\"../data/train/Audio/eric_excitement1/eric_excitement1.wav\")\n",
    "print(f\"File exists: {os.path.exists(audio_path)}\")\n",
    "print(f\"Absolute path: {os.path.abspath(audio_path)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T23:48:20.031580900Z",
     "start_time": "2025-03-31T23:48:20.015387600Z"
    }
   },
   "id": "1e4246a20eee3389"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd665a0-aad7-4c82-8e80-d49b69220088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T23:48:20.090887100Z",
     "start_time": "2025-03-31T23:48:20.021581300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 142\n",
      "Test dataset size: 61\n"
     ]
    }
   ],
   "source": [
    "emotion_classes = ['Rage', 'Excitement', 'Fear', 'Frustration']\n",
    "\n",
    "train_dataset = EmotionDataset(\n",
    "    root_dir=\"../data/train\",\n",
    "    transform=image_transform,\n",
    "    audio_transform=audio_transform,\n",
    ")\n",
    "\n",
    "test_dataset = EmotionDataset(\n",
    "    root_dir=\"../data/test\",\n",
    "    transform=image_transform,\n",
    "    audio_transform=audio_transform,\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6903a8a33ea71921"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31cfe87-66d5-46d3-b082-8ae212890f71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T23:48:20.461485800Z",
     "start_time": "2025-03-31T23:48:20.048890700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "video_model = VideoCNN(num_classes=len(emotion_classes)).to(device)\n",
    "audio_model = AudioCNN(num_classes=len(emotion_classes)).to(device)\n",
    "fusion_model = LateFusion(num_classes=len(emotion_classes), fusion_type='mlp').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define loss and optimizers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "151f20e5005ed979"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58915ec5-6670-421c-bdd6-a70e117024f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T23:48:20.462485Z",
     "start_time": "2025-03-31T23:48:20.455412100Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "video_optimizer = optim.Adam(video_model.parameters(), lr=0.001)\n",
    "audio_optimizer = optim.Adam(audio_model.parameters(), lr=0.001)\n",
    "fusion_optimizer = optim.Adam(fusion_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b5fff2bb83dfae1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_video_model(epochs=10):\n",
    "    video_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Video Epoch {epoch+1}\"):\n",
    "            audio, frames, labels = batch\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            video_optimizer.zero_grad()\n",
    "            outputs = video_model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            video_optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Video Model - Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "def train_audio_model(epochs=10):\n",
    "    audio_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Audio Epoch {epoch+1}\"):\n",
    "            audio, frames, labels = batch\n",
    "            audio = audio.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            audio_optimizer.zero_grad()\n",
    "            outputs = audio_model(audio)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            audio_optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Audio Model - Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T23:48:20.475482800Z",
     "start_time": "2025-03-31T23:48:20.465488700Z"
    }
   },
   "id": "6ff1259038444bf2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a6eb02fabe03127"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Video Epoch 1: 100%|██████████| 18/18 [00:12<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Model - Epoch 1: Loss: 2.0101, Acc: 29.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Video Epoch 2: 100%|██████████| 18/18 [00:11<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Model - Epoch 2: Loss: 1.2690, Acc: 44.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Video Epoch 3: 100%|██████████| 18/18 [00:11<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Model - Epoch 3: Loss: 1.1772, Acc: 46.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Video Epoch 4: 100%|██████████| 18/18 [00:11<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Model - Epoch 4: Loss: 1.1611, Acc: 48.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Video Epoch 5: 100%|██████████| 18/18 [00:11<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Model - Epoch 5: Loss: 1.2428, Acc: 44.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio Epoch 1: 100%|██████████| 18/18 [00:11<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Model - Epoch 1: Loss: 1.5497, Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio Epoch 2: 100%|██████████| 18/18 [00:11<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Model - Epoch 2: Loss: 1.1537, Acc: 57.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio Epoch 3: 100%|██████████| 18/18 [00:11<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Model - Epoch 3: Loss: 1.0464, Acc: 57.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio Epoch 4: 100%|██████████| 18/18 [00:11<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Model - Epoch 4: Loss: 0.9149, Acc: 72.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio Epoch 5: 100%|██████████| 18/18 [00:11<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Model - Epoch 5: Loss: 0.7733, Acc: 69.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_video_model(epochs=5)\n",
    "train_audio_model(epochs=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T23:50:39.771510200Z",
     "start_time": "2025-03-31T23:48:42.618425700Z"
    }
   },
   "id": "7c5554a11a24a80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f02343721d17e3a3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 61\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T23:53:09.857308200Z",
     "start_time": "2025-03-31T23:53:09.836933600Z"
    }
   },
   "id": "ad5e8f31ecc17462"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_models():\n",
    "    video_model.eval()\n",
    "    audio_model.eval()\n",
    "    fusion_model.eval()\n",
    "    \n",
    "    all_video_preds = []\n",
    "    all_audio_preds = []\n",
    "    all_fusion_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "            try:\n",
    "                # Unpack batch\n",
    "                audio, frames, labels = batch\n",
    "\n",
    "                # Sanity check\n",
    "                print(f\"Batch {batch_idx}: audio {audio.shape}, frames {frames.shape}, labels {labels.shape}\")\n",
    "\n",
    "                # Move to device\n",
    "                audio = audio.to(device)\n",
    "                frames = frames.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                video_outputs = video_model(frames)\n",
    "                audio_outputs = audio_model(audio)\n",
    "                fusion_outputs = fusion_model(video_outputs, audio_outputs)\n",
    "\n",
    "                # Predictions\n",
    "                _, video_preds = torch.max(video_outputs, dim=1)\n",
    "                _, audio_preds = torch.max(audio_outputs, dim=1)\n",
    "                _, fusion_preds = torch.max(fusion_outputs, dim=1)\n",
    "\n",
    "                # Collect results\n",
    "                all_video_preds.extend(video_preds.cpu().numpy())\n",
    "                all_audio_preds.extend(audio_preds.cpu().numpy())\n",
    "                all_fusion_preds.extend(fusion_preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Exception during evaluation (batch {batch_idx}): {e}\")\n",
    "                continue\n",
    "\n",
    "    # Print results only if we collected predictions\n",
    "    if all_labels:\n",
    "        print(\"\\n🎬 Video Model Results:\")\n",
    "        print(classification_report(all_labels, all_video_preds, target_names=emotion_classes))\n",
    "\n",
    "        print(\"\\n🔊 Audio Model Results:\")\n",
    "        print(classification_report(all_labels, all_audio_preds, target_names=emotion_classes))\n",
    "\n",
    "        print(\"\\n🤝 Fusion Model Results:\")\n",
    "        print(classification_report(all_labels, all_fusion_preds, target_names=emotion_classes))\n",
    "    else:\n",
    "        print(\"⚠️ No predictions collected. Check your DataLoader or model output.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:41.801695900Z",
     "start_time": "2025-04-01T00:00:41.778376500Z"
    }
   },
   "id": "7c6329a486c21a03"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: audio torch.Size([8, 1, 64, 64]), frames torch.Size([8, 20, 1, 112, 112]), labels torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|█▎        | 1/8 [00:07<00:54,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: audio torch.Size([8, 1, 64, 64]), frames torch.Size([8, 20, 1, 112, 112]), labels torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|███▊      | 3/8 [00:08<00:10,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2: audio torch.Size([8, 1, 64, 64]), frames torch.Size([8, 20, 1, 112, 112]), labels torch.Size([8])\n",
      "Batch 3: audio torch.Size([8, 1, 64, 64]), frames torch.Size([8, 20, 1, 112, 112]), labels torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  62%|██████▎   | 5/8 [00:08<00:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4: audio torch.Size([8, 1, 64, 64]), frames torch.Size([8, 20, 1, 112, 112]), labels torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  75%|███████▌  | 6/8 [00:09<00:02,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5: audio torch.Size([8, 1, 64, 64]), frames torch.Size([8, 20, 1, 112, 112]), labels torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  88%|████████▊ | 7/8 [00:10<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6: audio torch.Size([8, 1, 64, 64]), frames torch.Size([8, 20, 1, 112, 112]), labels torch.Size([8])\n",
      "Batch 7: audio torch.Size([5, 1, 64, 64]), frames torch.Size([5, 20, 1, 112, 112]), labels torch.Size([5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 8/8 [00:11<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎬 Video Model Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Rage       0.54      0.45      0.49        29\n",
      "  Excitement       0.31      0.69      0.43        16\n",
      "        Fear       0.00      0.00      0.00         9\n",
      " Frustration       1.00      0.14      0.25         7\n",
      "\n",
      "    accuracy                           0.41        61\n",
      "   macro avg       0.46      0.32      0.29        61\n",
      "weighted avg       0.45      0.41      0.38        61\n",
      "\n",
      "\n",
      "🔊 Audio Model Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Rage       0.54      0.69      0.61        29\n",
      "  Excitement       0.47      0.50      0.48        16\n",
      "        Fear       0.29      0.22      0.25         9\n",
      " Frustration       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.49        61\n",
      "   macro avg       0.32      0.35      0.34        61\n",
      "weighted avg       0.42      0.49      0.45        61\n",
      "\n",
      "\n",
      "🤝 Fusion Model Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Rage       0.44      0.24      0.31        29\n",
      "  Excitement       0.00      0.00      0.00        16\n",
      "        Fear       0.00      0.00      0.00         9\n",
      " Frustration       0.13      0.86      0.23         7\n",
      "\n",
      "    accuracy                           0.21        61\n",
      "   macro avg       0.14      0.27      0.14        61\n",
      "weighted avg       0.22      0.21      0.17        61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate_models()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T00:01:06.395504500Z",
     "start_time": "2025-04-01T00:00:55.246912300Z"
    }
   },
   "id": "e18126eaa9da8ca0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
