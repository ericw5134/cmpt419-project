{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d23fb55-9e80-44ab-9872-5168e88c4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecce9a-a70c-4f75-8d23-b20f26871783",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b78e8f-4da0-4af9-b004-2095fa92ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, image_dir, audio_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.audio_files = sorted(os.listdir(audio_dir))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        audio_path = os.path.join(self.audio_dir, self.audio_files[idx])\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=64)(waveform)\n",
    "        \n",
    "        return image, mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd665a0-aad7-4c82-8e80-d49b69220088",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'EmotionDataset' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m trainRage_dataset = EmotionDataset(\u001b[33m\"\u001b[39m\u001b[33mdata/train/trainRage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata/train/trainAudio\u001b[39m\u001b[33m\"\u001b[39m, transform=image_transform)\n\u001b[32m      9\u001b[39m testRage_dataset = EmotionDataset(\u001b[33m\"\u001b[39m\u001b[33mdata/test/testRage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata/test/testAudio\u001b[39m\u001b[33m\"\u001b[39m, transform=image_transform)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m trainRage_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainRage_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m testRage_loader = DataLoader(testRage_dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img, audio \u001b[38;5;129;01min\u001b[39;00m train_loader:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    352\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:142\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.replacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:149\u001b[39m, in \u001b[36mRandomSampler.num_samples\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_samples\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'EmotionDataset' has no len()"
     ]
    }
   ],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainRage_dataset = EmotionDataset(\"data/train/trainRage\", \"data/train/trainAudio\", transform=image_transform)\n",
    "testRage_dataset = EmotionDataset(\"data/test/testRage\", \"data/test/testAudio\", transform=image_transform)\n",
    "\n",
    "trainRage_loader = DataLoader(trainRage_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "testRage_loader = DataLoader(testRage_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "for img, audio in train_loader:\n",
    "    print(\"image shape:\", img.shape)  \n",
    "    print(\"audio shape:\", audio.shape)  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31cfe87-66d5-46d3-b082-8ae212890f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58915ec5-6670-421c-bdd6-a70e117024f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
